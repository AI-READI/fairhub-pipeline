"""
Manifest Validation Script

This script validates the manifest data generated by local_imaging_manifest.py.
It checks:
1. Manifest file existence and structure
2. File path validity and accessibility
3. DICOM file integrity
4. Metadata consistency and required fields
5. Cross-references between manifests

Usage:
    python validate_manifests.py [--input-folder PATH] [--manifest-folder PATH] [--verbose]
"""

import sys
import logging
import argparse
import pandas as pd
import pydicom
from pathlib import Path
from typing import Dict
from collections import defaultdict
from tqdm import tqdm

# Set up logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class ManifestValidator:
    """Validates imaging manifests and their associated data files."""

    def __init__(self, input_folder: str, manifest_folder: str, verbose: bool = False):
        """
        Initialize the validator.

        Args:
            input_folder: Path to the input data folder (e.g., Spectralis-processed)
            manifest_folder: Path to the manifest output folder (e.g., Spectralis-manifests)
            verbose: Enable verbose logging
        """
        self.input_folder = Path(input_folder)
        self.manifest_folder = Path(manifest_folder)
        self.verbose = verbose

        if verbose:
            logging.getLogger().setLevel(logging.DEBUG)

        # Validation results
        self.errors = []
        self.warnings = []
        self.stats = defaultdict(int)

        # Expected manifest structure
        self.expected_manifests = {
            "retinal_photography": {
                "required_columns": [
                    "person_id",
                    "manufacturer",
                    "manufacturers_model_name",
                    "laterality",
                    "anatomic_region",
                    "imaging",
                    "height",
                    "width",
                    "color_channel_dimension",
                    "sop_instance_uid",
                    "filepath",
                ],
                "data_folder": "retinal_photography",
            },
            "retinal_oct": {
                "required_columns": [
                    "person_id",
                    "manufacturer",
                    "manufacturers_model_name",
                    "anatomic_region",
                    "imaging",
                    "laterality",
                    "height",
                    "width",
                    "number_of_frames",
                    "pixel_spacing",
                    "slice_thickness",
                    "sop_instance_uid",
                    "filepath",
                    "reference_instance_uid",
                    "reference_filepath",
                ],
                "data_folder": "retinal_oct",
            },
            "retinal_octa": {
                "required_columns": [
                    "person_id",
                    "manufacturer",
                    "manufacturers_model_name",
                    "anatomic_region",
                    "imaging",
                    "laterality",
                    "flow_cube_height",
                    "flow_cube_width",
                    "flow_cube_number_of_frames",
                    "flow_cube_sop_instance_uid",
                    "flow_cube_file_path",
                ],
                "data_folder": "retinal_octa",
            },
        }

    def validate_all(self) -> bool:
        """
        Run all validation checks.

        Returns:
            bool: True if all validations pass, False otherwise
        """
        logger.info("Starting comprehensive manifest validation")
        logger.info(f"Input folder: {self.input_folder}")
        logger.info(f"Manifest folder: {self.manifest_folder}")

        # Check if folders exist
        if not self._validate_folders():
            return False

        # Validate each manifest
        all_valid = True
        for manifest_type in tqdm(
            self.expected_manifests.keys(), desc="Validating manifests", unit="manifest"
        ):
            logger.info(f"\n{'=' * 50}")
            logger.info(f"Validating {manifest_type} manifest")
            logger.info(f"{'=' * 50}")

            if not self._validate_manifest(manifest_type):
                all_valid = False

        # Validate cross-references between manifests
        logger.info(f"\n{'=' * 50}")
        logger.info("Validating cross-references between manifests")
        logger.info(f"{'=' * 50}")

        if not self._validate_cross_references():
            all_valid = False

        # Print summary
        self._print_summary()

        return all_valid

    def _validate_folders(self) -> bool:
        """Validate that required folders exist."""
        valid = True

        if not self.input_folder.exists():
            self.errors.append(f"Input folder does not exist: {self.input_folder}")
            valid = False
        else:
            logger.info(f"✓ Input folder exists: {self.input_folder}")

        if not self.manifest_folder.exists():
            self.errors.append(
                f"Manifest folder does not exist: {self.manifest_folder}"
            )
            valid = False
        else:
            logger.info(f"✓ Manifest folder exists: {self.manifest_folder}")

        return valid

    def _validate_manifest(self, manifest_type: str) -> bool:
        """Validate a specific manifest file."""
        manifest_path = self.manifest_folder / manifest_type / "manifest.tsv"

        if not manifest_path.exists():
            self.errors.append(f"Manifest file not found: {manifest_path}")
            return False

        logger.info(f"✓ Found manifest file: {manifest_path}")

        try:
            # Load manifest
            df = pd.read_csv(manifest_path, sep="\t")
            self.stats[f"{manifest_type}_rows"] = len(df)

            # Validate structure
            if not self._validate_manifest_structure(df, manifest_type):
                return False

            # Validate file paths
            if not self._validate_file_paths(df, manifest_type):
                return False

            # Validate DICOM files
            if not self._validate_dicom_files(df, manifest_type):
                return False

            # Validate metadata consistency
            if not self._validate_metadata_consistency(df, manifest_type):
                return False

            logger.info(f"✓ {manifest_type} manifest validation completed")
            return True

        except Exception as e:
            self.errors.append(f"Error reading manifest {manifest_path}: {str(e)}")
            return False

    def _validate_manifest_structure(
        self, df: pd.DataFrame, manifest_type: str
    ) -> bool:
        """Validate manifest structure and required columns."""
        valid = True
        expected_columns = self.expected_manifests[manifest_type]["required_columns"]

        # Check for required columns
        missing_columns = set(expected_columns) - set(df.columns)
        if missing_columns:
            self.errors.append(
                f"{manifest_type}: Missing required columns: {missing_columns}"
            )
            valid = False
        else:
            logger.info(f"✓ {manifest_type}: All required columns present")

        # Check for empty manifest
        if len(df) == 0:
            self.warnings.append(f"{manifest_type}: Manifest is empty")
        else:
            logger.info(f"✓ {manifest_type}: Manifest contains {len(df)} rows")

        return valid

    def _validate_file_paths(self, df: pd.DataFrame, manifest_type: str) -> bool:
        """Validate that file paths in manifest exist."""
        valid = True
        data_folder = (
            self.input_folder / self.expected_manifests[manifest_type]["data_folder"]
        )

        # Get filepath column name (varies by manifest type)
        filepath_columns = [
            col
            for col in df.columns
            if "filepath" in col.lower() or "file_path" in col.lower()
        ]

        for filepath_col in filepath_columns:
            missing_files = []
            filepath_data = df[filepath_col].dropna()

            for idx, filepath in tqdm(
                filepath_data.items(),
                desc=f"Validating {filepath_col} files",
                unit="file",
                leave=False,
            ):
                if pd.isna(filepath) or filepath == "Not Reported":
                    continue

                # Convert relative path to absolute path
                if filepath.startswith(data_folder.name):
                    # Path is relative to data folder
                    full_path = self.input_folder / filepath
                else:
                    # Path might be relative to input folder
                    full_path = self.input_folder / filepath

                if not full_path.exists():
                    missing_files.append((idx, str(filepath)))
                    self.stats[f"{manifest_type}_missing_files"] += 1
                else:
                    self.stats[f"{manifest_type}_valid_files"] += 1

            if missing_files:
                self.errors.append(
                    f"{manifest_type}: {len(missing_files)} files not found in {filepath_col}"
                )
                if self.verbose:
                    for idx, filepath in missing_files[:5]:  # Show first 5
                        self.errors.append(f"  Row {idx}: {filepath}")
                valid = False
            else:
                logger.info(f"✓ {manifest_type}: All files in {filepath_col} exist")

        return valid

    def _validate_dicom_files(self, df: pd.DataFrame, manifest_type: str) -> bool:
        """Validate DICOM file integrity."""
        valid = True
        data_folder = (
            self.input_folder / self.expected_manifests[manifest_type]["data_folder"]
        )

        # Get filepath column name
        filepath_columns = [
            col
            for col in df.columns
            if "filepath" in col.lower() or "file_path" in col.lower()
        ]

        for filepath_col in filepath_columns:
            corrupted_files = []
            filepath_data = df[filepath_col].dropna()

            for idx, filepath in tqdm(
                filepath_data.items(),
                desc=f"Validating DICOM files in {filepath_col}",
                unit="file",
                leave=False,
            ):
                if pd.isna(filepath) or filepath == "Not Reported":
                    continue

                # Convert to full path
                if filepath.startswith(data_folder.name):
                    full_path = self.input_folder / filepath
                else:
                    full_path = self.input_folder / filepath

                if full_path.exists():
                    try:
                        # Try to read DICOM file
                        dicom_data = pydicom.dcmread(full_path)

                        # Basic DICOM validation
                        if not hasattr(dicom_data, "SOPInstanceUID"):
                            corrupted_files.append(
                                (idx, str(filepath), "Missing SOPInstanceUID")
                            )
                        elif not hasattr(dicom_data, "PatientID"):
                            corrupted_files.append(
                                (idx, str(filepath), "Missing PatientID")
                            )
                        else:
                            self.stats[f"{manifest_type}_valid_dicom"] += 1

                    except Exception as e:
                        corrupted_files.append((idx, str(filepath), str(e)))
                        self.stats[f"{manifest_type}_corrupted_dicom"] += 1

            if corrupted_files:
                self.errors.append(
                    f"{manifest_type}: {len(corrupted_files)} corrupted DICOM files in {filepath_col}"
                )
                if self.verbose:
                    for idx, filepath, error in corrupted_files[:5]:  # Show first 5
                        self.errors.append(f"  Row {idx}: {filepath} - {error}")
                valid = False
            else:
                logger.info(
                    f"✓ {manifest_type}: All DICOM files in {filepath_col} are valid"
                )

        return valid

    def _validate_metadata_consistency(
        self, df: pd.DataFrame, manifest_type: str
    ) -> bool:
        """Validate metadata consistency within manifest."""
        valid = True

        # Check for required fields
        required_fields = ["person_id", "sop_instance_uid"]
        for field in required_fields:
            if field in df.columns:
                null_count = df[field].isnull().sum()
                if null_count > 0:
                    self.errors.append(
                        f"{manifest_type}: {null_count} rows with null {field}"
                    )
                    valid = False
                else:
                    logger.info(f"✓ {manifest_type}: All {field} values present")

        # Check for duplicate SOP Instance UIDs
        if "sop_instance_uid" in df.columns:
            duplicates = df["sop_instance_uid"].duplicated().sum()
            if duplicates > 0:
                self.errors.append(
                    f"{manifest_type}: {duplicates} duplicate SOP Instance UIDs"
                )
                valid = False
            else:
                logger.info(f"✓ {manifest_type}: No duplicate SOP Instance UIDs")

        # Validate specific fields based on manifest type
        if manifest_type == "retinal_photography":
            if not self._validate_photography_metadata(df):
                valid = False
        elif manifest_type == "retinal_oct":
            if not self._validate_oct_metadata(df):
                valid = False
        elif manifest_type == "retinal_octa":
            if not self._validate_octa_metadata(df):
                valid = False

        return valid

    def _validate_photography_metadata(self, df: pd.DataFrame) -> bool:
        """Validate retinal photography specific metadata."""
        valid = True

        # Check manufacturer
        if "manufacturer" in df.columns:
            invalid_manufacturer = df[df["manufacturer"] != "Heidelberg"]
            if len(invalid_manufacturer) > 0:
                self.warnings.append(
                    f"retinal_photography: {len(invalid_manufacturer)} rows with non-Heidelberg manufacturer"
                )

        # Check imaging type
        if "imaging" in df.columns:
            invalid_imaging = df[df["imaging"] != "Infrared Reflectance"]
            if len(invalid_imaging) > 0:
                self.warnings.append(
                    f"retinal_photography: {len(invalid_imaging)} rows with non-IR imaging type"
                )

        return valid

    def _validate_oct_metadata(self, df: pd.DataFrame) -> bool:
        """Validate retinal OCT specific metadata."""
        valid = True

        # Check imaging type
        if "imaging" in df.columns:
            invalid_imaging = df[df["imaging"] != "OCT"]
            if len(invalid_imaging) > 0:
                self.warnings.append(
                    f"retinal_oct: {len(invalid_imaging)} rows with non-OCT imaging type"
                )

        # Check for required numeric fields
        numeric_fields = ["height", "width", "number_of_frames"]
        for field in numeric_fields:
            if field in df.columns:
                non_numeric = df[~pd.to_numeric(df[field], errors="coerce").notna()]
                if len(non_numeric) > 0:
                    self.errors.append(
                        f"retinal_oct: {len(non_numeric)} rows with non-numeric {field}"
                    )
                    valid = False

        return valid

    def _validate_octa_metadata(self, df: pd.DataFrame) -> bool:
        """Validate retinal OCTA specific metadata."""
        valid = True

        # Check imaging type
        if "imaging" in df.columns:
            invalid_imaging = df[df["imaging"] != "OCTA"]
            if len(invalid_imaging) > 0:
                self.warnings.append(
                    f"retinal_octa: {len(invalid_imaging)} rows with non-OCTA imaging type"
                )

        # Check for required numeric fields
        numeric_fields = [
            "flow_cube_height",
            "flow_cube_width",
            "flow_cube_number_of_frames",
        ]
        for field in numeric_fields:
            if field in df.columns:
                non_numeric = df[~pd.to_numeric(df[field], errors="coerce").notna()]
                if len(non_numeric) > 0:
                    self.errors.append(
                        f"retinal_octa: {len(non_numeric)} rows with non-numeric {field}"
                    )
                    valid = False

        return valid

    def _validate_cross_references(self) -> bool:
        """Validate cross-references between manifests."""
        valid = True

        try:
            # Load all manifests
            manifests = {}
            for manifest_type in self.expected_manifests.keys():
                manifest_path = self.manifest_folder / manifest_type / "manifest.tsv"
                if manifest_path.exists():
                    manifests[manifest_type] = pd.read_csv(manifest_path, sep="\t")

            # Validate OCT references to photography
            if "retinal_oct" in manifests and "retinal_photography" in manifests:
                if not self._validate_oct_photography_references(manifests):
                    valid = False

            # Validate OCTA references
            if "retinal_octa" in manifests:
                if not self._validate_octa_references(manifests):
                    valid = False

            return valid

        except Exception as e:
            self.errors.append(f"Error validating cross-references: {str(e)}")
            return False

    def _validate_oct_photography_references(
        self, manifests: Dict[str, pd.DataFrame]
    ) -> bool:
        """Validate OCT references to photography files."""
        valid = True
        oct_df = manifests["retinal_oct"]
        photo_df = manifests["retinal_photography"]

        if (
            "reference_instance_uid" in oct_df.columns
            and "sop_instance_uid" in photo_df.columns
        ):
            photo_uids = set(photo_df["sop_instance_uid"].dropna())
            missing_refs = []
            ref_data = oct_df["reference_instance_uid"].dropna()

            for idx, ref_uid in tqdm(
                ref_data.items(),
                desc="Validating OCT photography references",
                unit="reference",
                leave=False,
            ):
                if ref_uid not in photo_uids:
                    missing_refs.append((idx, ref_uid))

            if missing_refs:
                self.errors.append(
                    f"retinal_oct: {len(missing_refs)} rows with invalid photography references"
                )
                if self.verbose:
                    for idx, ref_uid in missing_refs[:5]:
                        self.errors.append(
                            f"  Row {idx}: Reference UID {ref_uid} not found in photography manifest"
                        )
                valid = False
            else:
                logger.info("✓ retinal_oct: All photography references valid")

        return valid

    def _validate_octa_references(self, manifests: Dict[str, pd.DataFrame]) -> bool:
        """Validate OCTA references to other manifests."""
        valid = True
        octa_df = manifests["retinal_octa"]

        # Check photography references
        if "retinal_photography" in manifests:
            photo_df = manifests["retinal_photography"]
            if "associated_retinal_photography_sop_instance_uid" in octa_df.columns:
                photo_uids = set(photo_df["sop_instance_uid"].dropna())
                missing_refs = []
                ref_data = octa_df[
                    "associated_retinal_photography_sop_instance_uid"
                ].dropna()

                for idx, ref_uid in tqdm(
                    ref_data.items(),
                    desc="Validating OCTA photography references",
                    unit="reference",
                    leave=False,
                ):
                    if ref_uid != "Not Reported" and ref_uid not in photo_uids:
                        missing_refs.append((idx, ref_uid))

                if missing_refs:
                    self.errors.append(
                        f"retinal_octa: {len(missing_refs)} rows with invalid photography references"
                    )
                    valid = False
                else:
                    logger.info("✓ retinal_octa: All photography references valid")

        # Check OCT references
        if "retinal_oct" in manifests:
            oct_df = manifests["retinal_oct"]
            if "associated_structural_oct_sop_instance_uid" in octa_df.columns:
                oct_uids = set(oct_df["sop_instance_uid"].dropna())
                missing_refs = []
                ref_data = octa_df[
                    "associated_structural_oct_sop_instance_uid"
                ].dropna()

                for idx, ref_uid in tqdm(
                    ref_data.items(),
                    desc="Validating OCTA OCT references",
                    unit="reference",
                    leave=False,
                ):
                    if ref_uid != "Not Reported" and ref_uid not in oct_uids:
                        missing_refs.append((idx, ref_uid))
                        print(
                            f"Missing reference: {ref_uid} for row {idx} for OCTA OCT references"
                        )
                        print(f"OCTA OCT references: {oct_uids}")
                        print(
                            f"OCT references: {oct_df['sop_instance_uid'].dropna().tolist()}"
                        )

                if missing_refs:
                    self.errors.append(
                        f"retinal_octa: {len(missing_refs)} rows with invalid OCT references"
                    )
                    valid = False
                else:
                    logger.info("✓ retinal_octa: All OCT references valid")

        return valid

    def _print_summary(self):
        """Print validation summary."""
        logger.info("\n" + "=" * 60)
        logger.info("VALIDATION SUMMARY")
        logger.info("=" * 60)

        # Print statistics
        logger.info("\nStatistics:")
        for key, value in self.stats.items():
            logger.info(f"  {key}: {value}")

        # Print errors
        if self.errors:
            logger.error(f"\nERRORS ({len(self.errors)}):")
            for error in self.errors:
                logger.error(f"  ❌ {error}")
        else:
            logger.info("\n✓ No errors found")

        # Print warnings
        if self.warnings:
            logger.warning(f"\nWARNINGS ({len(self.warnings)}):")
            for warning in self.warnings:
                logger.warning(f"  ⚠️  {warning}")
        else:
            logger.info("\n✓ No warnings found")

        # Overall result
        if self.errors:
            logger.error(f"\n❌ VALIDATION FAILED: {len(self.errors)} errors found")
        else:
            logger.info("\n✅ VALIDATION PASSED: All checks completed successfully")


def main():
    """Main function to run validation."""
    parser = argparse.ArgumentParser(description="Validate imaging manifests")
    parser.add_argument(
        "--input-folder",
        default=r"C:\Users\sanjay\Downloads\Spectralis-processed",
        help="Path to input data folder",
    )
    parser.add_argument(
        "--manifest-folder",
        default=r"C:\Users\sanjay\Downloads\Spectralis-manifests",
        help="Path to manifest output folder",
    )
    parser.add_argument("--verbose", action="store_true", help="Enable verbose logging")

    args = parser.parse_args()

    # Create validator and run validation
    validator = ManifestValidator(
        input_folder=args.input_folder,
        manifest_folder=args.manifest_folder,
        verbose=args.verbose,
    )

    success = validator.validate_all()

    # Exit with appropriate code
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
